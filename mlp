import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import time
from datetime import timedelta
from pathlib import Path

# 1) Data preparation (English only, no visualization)
print("1) Preparing data…")

# Use project-relative paths for portability
DATA_DIR = Path(__file__).parent / 'data'
file_path_te = DATA_DIR / 'TE.csv'  # TE mode diffraction efficiency
file_path_tm = DATA_DIR / 'TM.csv'  # TM mode diffraction efficiency

if not file_path_te.exists() or not file_path_tm.exists():
    raise FileNotFoundError(
        f"Missing data files. Expected at: '{file_path_te}' and '{file_path_tm}'."
    )

print("Reading TE data…")
df_te = pd.read_csv(file_path_te, encoding_errors='ignore')
print(f"TE shape: {df_te.shape}")

print("Reading TM data…")
df_tm = pd.read_csv(file_path_tm, encoding_errors='ignore')
print(f"TM shape: {df_tm.shape}")

# Validate matching rows
if df_te.shape[0] != df_tm.shape[0]:
    raise ValueError(f"Row count mismatch between TE and TM: {df_te.shape[0]} vs {df_tm.shape[0]}")

# First 4 columns as input features (assume same column order)
X = df_te.iloc[:, :4].values

# Fifth column from both files as dual targets: [TE, TM]
y_te = df_te.iloc[:, 4].values.reshape(-1, 1)
y_tm = df_tm.iloc[:, 4].values.reshape(-1, 1)
y = np.column_stack([y_te.flatten(), y_tm.flatten()])

print(f"Features shape: {X.shape}")
print(f"Targets shape:  {y.shape} (TE, TM)")

# Standardize features
scaler_X = StandardScaler()
X = scaler_X.fit_transform(X)

# To tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# Dataset split (fixed train size = 1000; adjust if dataset smaller)
dataset = TensorDataset(X_tensor, y_tensor)
train_size = min(1000, len(X_tensor) - 1) if len(X_tensor) > 1 else 0
test_size = len(X_tensor) - train_size
if train_size == 0 or test_size == 0:
    raise ValueError("Dataset too small to split into train/test. Need at least 2 samples.")
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
X_train, y_train = train_dataset[:][0].to(device), train_dataset[:][1].to(device)
X_test, y_test = test_dataset[:][0].to(device), test_dataset[:][1].to(device)

print("Split summary:")
print(f"  Total:   {len(X_tensor)}")
print(f"  Train:   {train_size}")
print(f"  Test:    {test_size}")
print(f"  Train %: {train_size / len(X_tensor) * 100:.1f}%")


# 2) Model definition - Simple MLP for dual outputs (TE, TM)
class SimpleMLP(nn.Module):
    def __init__(self, input_dim, hidden_dims=[128, 256, 512, 256, 128], dropout=0.1):
        super().__init__()

        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout),
            ])
            prev_dim = hidden_dim

        # Output layer -> 2 (TE, TM)
        layers.append(nn.Linear(prev_dim, 2))
        self.mlp = nn.Sequential(*layers)

        self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.mlp(x)  # [batch_size, 2]


print("\n2) Initializing MLP model…")
model = SimpleMLP(
    input_dim=X_train.shape[1],
    hidden_dims=[128, 256, 512, 256, 128],
    dropout=0.1,
).to(device)

total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Trainable parameters: {total_params:,}")

# 3) Training config
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)

# 4) Training loop (no plots; prints minimal progress)
print("\n3) Start training (progress by batches)…")
best_val_loss = float('inf')
num_epochs = 200
total_steps = num_epochs * len(train_loader)
start_time = time.time()

progress_bar = tqdm(total=total_steps, desc="Training", unit="batch")

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0

    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

        # Update progress bar
        elapsed = time.time() - start_time
        steps_completed = epoch * len(train_loader) + 1
        samples_processed = steps_completed * train_loader.batch_size
        samples_per_sec = samples_processed / elapsed if elapsed > 0 else 0.0
        progress_bar.update(1)
        progress_bar.set_postfix({
            'Epoch': f'{epoch + 1}/{num_epochs}',
            'Loss': f'{loss.item():.4f}',
            'Samples/sec': f'{samples_per_sec:.1f}',
            'Elapsed': str(timedelta(seconds=int(elapsed))),
        })

    # Validation on the held-out set
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_test)
        val_loss = criterion(val_outputs, y_test).item()

    scheduler.step(val_loss)

    # Save the best checkpoint by validation loss
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_simple_mlp.pth')

    # Print per-epoch summary (no Chinese)
    print(f"Epoch {epoch + 1:03d}/{num_epochs} | train_loss={epoch_loss / len(train_loader):.4f} | val_loss={val_loss:.4f}")

progress_bar.close()

print("\n4) Training finished. Best checkpoint saved to 'best_simple_mlp.pth'.")

