import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import time
from datetime import timedelta
import pickle
from pathlib import Path

# 1) Data preparation (English only, no visualization)
print("1) Preparing data…")

# Use project-relative paths for portability
DATA_DIR = Path(__file__).parent / 'data'
file_path_te = DATA_DIR / 'TE.csv'  # TE mode target in 5th column (index 4)
file_path_tm = DATA_DIR / 'TM.csv'  # TM mode target in 5th column (index 4)

if not file_path_te.exists() or not file_path_tm.exists():
    raise FileNotFoundError(
        f"Missing data files. Expected at: '{file_path_te}' and '{file_path_tm}'."
    )

print("Reading TE data…")
df_te = pd.read_csv(file_path_te, encoding_errors='ignore')
print(f"TE shape: {df_te.shape}")

print("Reading TM data…")
df_tm = pd.read_csv(file_path_tm, encoding_errors='ignore')
print(f"TM shape: {df_tm.shape}")

# Validate matching rows
if df_te.shape[0] != df_tm.shape[0]:
    raise ValueError(f"Row count mismatch between TE and TM: {df_te.shape[0]} vs {df_tm.shape[0]}")

# Basic column count check
min_required_cols = 5  # first 4 feature columns + at least 1 target column
if df_te.shape[1] < min_required_cols:
    raise ValueError(f'TE has too few columns; need >= {min_required_cols}, got {df_te.shape[1]}')
if df_tm.shape[1] < min_required_cols:
    raise ValueError(f'TM has too few columns; need >= {min_required_cols}, got {df_tm.shape[1]}')

# First four columns as input features
X = df_te.iloc[:, :4].values

# Select target column: prefer 5th column (index 4); fallback to last valid col with index >= 4
def _select_target_col(df, prefer_idx=4):
    ncol = df.shape[1]
    if ncol > prefer_idx and df.iloc[:, prefer_idx].notna().any():
        return prefer_idx
    for idx in range(ncol - 1, prefer_idx - 1, -1):
        if df.iloc[:, idx].notna().any():
            return idx
    raise ValueError('No valid target column found (5th or any to its right).')

te_col_idx = _select_target_col(df_te, prefer_idx=4)
tm_col_idx = _select_target_col(df_tm, prefer_idx=4)
print(f"TE target column -> index {te_col_idx}, name: {df_te.columns[te_col_idx]}")
print(f"TM target column -> index {tm_col_idx}, name: {df_tm.columns[tm_col_idx]}")

y_te = df_te.iloc[:, te_col_idx].values.reshape(-1, 1)
y_tm = df_tm.iloc[:, tm_col_idx].values.reshape(-1, 1)
y = np.column_stack([y_te.flatten(), y_tm.flatten()])  # dual output [TE, TM]

print(f"Features shape: {X.shape}")
print(f"Targets shape:  {y.shape} (TE, TM)")

# Standardize features
scaler_X = StandardScaler()
X = scaler_X.fit_transform(X)

# To tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# Dataset split (cap train size; robust for small datasets)
dataset = TensorDataset(X_tensor, y_tensor)
if len(X_tensor) < 2:
    raise ValueError("Dataset too small to split into train/test. Need at least 2 samples.")
train_size = min(6000, len(X_tensor) - 1)
test_size = len(X_tensor) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])
X_train, y_train = train_dataset[:][0].to(device), train_dataset[:][1].to(device)
X_test, y_test = test_dataset[:][0].to(device), test_dataset[:][1].to(device)

print("Split summary:")
print(f"  Total:   {len(X_tensor)}")
print(f"  Train:   {train_size}")
print(f"  Test:    {test_size}")
print(f"  Train %: {train_size / len(X_tensor) * 100:.1f}%")


# 2. ResNet模型定义 - 双输出CNN网络
class BasicBlock(nn.Module):
    """ResNet basic block (Linear + BN + ReLU with residual)."""

    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Linear(in_channels, out_channels)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Linear(out_channels, out_channels)
        self.bn2 = nn.BatchNorm1d(out_channels)

        # Residual connection path
        self.shortcut = nn.Sequential()
        if in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Linear(in_channels, out_channels),
                nn.BatchNorm1d(out_channels)
            )

    def forward(self, x):
        residual = self.shortcut(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += residual
        out = self.relu(out)

        return out


class DualOutputResNet(nn.Module):
    """基于ResNet的双输出网络"""

    def __init__(self, input_dim, num_classes=2):
        super(DualOutputResNet, self).__init__()

        # 输入层
        self.input_layer = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True)
        )

        # ResNet层
        self.layer1 = self._make_layer(64, 64, 2)
        self.layer2 = self._make_layer(64, 128, 2)
        self.layer3 = self._make_layer(128, 256, 2)
        self.layer4 = self._make_layer(256, 512, 2)

        # 全局平均池化
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)

        # 双输出分支 - TE模式输出
        self.te_output = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()  # 限制输出在0-1之间
        )

        # 双输出分支 - TM模式输出
        self.tm_output = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 1),
            nn.Sigmoid()  # 限制输出在0-1之间
        )

    def _make_layer(self, in_channels, out_channels, num_blocks):
        layers = []
        # 第一个块可能需要改变通道数
        layers.append(BasicBlock(in_channels, out_channels))
        # 其余块保持相同通道数
        for _ in range(1, num_blocks):
            layers.append(BasicBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
    # Input projection
        x = self.input_layer(x)

    # ResNet feature extraction
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

    # Dual output heads
        te_out = self.te_output(x)
        tm_out = self.tm_output(x)

    # Concatenate outputs -> [B, 2]
        return torch.cat([te_out, tm_out], dim=1)


print("\n2) Initializing dual-output ResNet model…")
model = DualOutputResNet(
    input_dim=X_train.shape[1]
).to(device)

# Parameter counts
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total parameters:     {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# 3) Training config
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)

# 4) Training loop (no plots; minimal English logs)
print("\n3) Start training (progress by batches)…")
best_val_loss = float('inf')
num_epochs = 200
total_steps = num_epochs * len(train_loader)
start_time = time.time()

progress_bar = tqdm(total=total_steps, desc="Training", unit="batch")

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0

    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

        # Update progress bar
        progress_bar.update(1)
        elapsed_time = time.time() - start_time
        progress_bar.set_postfix({
            'Epoch': f'{epoch + 1}/{num_epochs}',
            'Loss': f'{loss.item():.6f}',
            'Time': str(timedelta(seconds=int(elapsed_time)))
        })

    # Validation
    model.eval()
    with torch.no_grad():
        val_outputs = model(X_test)
        val_loss = criterion(val_outputs, y_test).item()

    scheduler.step(val_loss)

    # Save best checkpoint
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_resnet_model.pth')

    # Per-epoch summary
    print(f"Epoch {epoch + 1:03d}/{num_epochs} | train_loss={epoch_loss / len(train_loader):.6f} | val_loss={val_loss:.6f}")

# Save scaler
with open('scaler_X_resnet.pkl', 'wb') as f:
    pickle.dump(scaler_X, f)
print("Scaler saved to 'scaler_X_resnet.pkl'.")

progress_bar.close()


print("\n4) Training finished. Best checkpoint saved to 'best_resnet_model.pth'.")
